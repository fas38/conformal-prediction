## Data Preparation for Sepsis Prediction

In this study, we utilized the Sepsis dataset to evaluate our predictive models. The dataset contains detailed records of patient health metrics and diagnostic information crucial for accurate sepsis prediction. It consists of 995 samples, with the target variable indicating the presence of sepsis. Notably, the dataset is imbalanced, with 897 of the samples labeled as not having sepsis.

Before employing machine learning models and conformal prediction techniques for uncertainty quantification, the data underwent several preprocessing steps to ensure cleanliness and suitability for analysis. These steps included data cleaning, transformation and splitting.

The dataset contains a total of 57 features, including both continuous and categorical variables. Continuous variables such as age, C-reactive protein (CRP) levels, lactic acid levels (lacticacid), and white blood cell counts (leucocytes) provide quantitative insights essential for monitoring patient health and identifying sepsis. Categorical data includes diagnostic codes and binary indicators for various diagnostic tests such as blood cultures (diagnosticblood), ECGs (diagnosticecg), and X-rays (diagnosticxthorax), which are vital for clinical assessments. The dataset also contains sequential medical events and the timing of clinical interventions, providing a comprehensive view of each patient's treatment pathway.

The feature 'diagnose' contains missing values. As this feature is categorical, we addressed the missing data by converting all missing values to a single category labeled 'missing'. We transformed categorical and boolean features into numerical formats suitable for machine learning algorithms using the label encoder from the sklearn library.

For the continuous variables, we applied robust scaling using the robust scaler method from sklearn. We selected robust scaling for its effectiveness in mitigating the influence of outliers by using the median and interquartile range for scaling.

The dataset is divided into training and testing sets with an 80-20 split to ensure representative sampling from the overall population. We additionally split the training set to create a calibration set, which accounts for 20% of the training data and is used for uncertainty quantification via conformal prediction. The remaining 80% of the training data was used to train the models. Throughout this process, we employed stratified sampling to maintain consistent distribution of the target variable across the training, calibration, and testing sets.